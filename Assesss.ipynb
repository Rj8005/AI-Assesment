{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Problem 1"
      ],
      "metadata": {
        "id": "DOnR02CE0OLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch fastapi uvicorn pyngrok requests beautifulsoup4\n"
      ],
      "metadata": {
        "id": "igdHPP_ukXWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import pipeline\n",
        "import uvicorn\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "import socket\n",
        "\n",
        "# Function to verify if a port is in use\n",
        "def is_port_in_use(port):\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        return s.connect_ex(('localhost', port)) == 0\n",
        "\n",
        "port = 8002\n",
        "if is_port_in_use(port):\n",
        "    print(f\"Port {port} is already in use. Try stopping conflicting processes or choose another port.\")\n",
        "else:\n",
        "    # Initialize the Hugging Face QA model pipeline\n",
        "    qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
        "    def answer_question(context, question):\n",
        "        result = qa_pipeline({\"context\": context, \"question\": question})\n",
        "        return result.get(\"answer\", \"I don't know the answer\")\n",
        "\n",
        "    # Initialize the FastAPI application\n",
        "    app = FastAPI()\n",
        "\n",
        "    # Define the data model for incoming requests\n",
        "    class QARequest(BaseModel):\n",
        "        url: str\n",
        "        question: str\n",
        "\n",
        "    # Endpoint to answer questions based on webpage content\n",
        "    @app.post('/question_answering')\n",
        "    def answer_question_from_web(data: QARequest):\n",
        "        # Fetch webpage content\n",
        "        try:\n",
        "            response = requests.get(data.url)\n",
        "            response.raise_for_status()\n",
        "        except requests.RequestException as e:\n",
        "            raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "        # Parse the content using BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        page_content = soup.get_text(separator=' ')\n",
        "\n",
        "        # Answer the question using the Hugging Face pipeline\n",
        "        result = answer_question(page_content, data.question)\n",
        "        if result == \"I don't know the answer\" or not result:\n",
        "            return {\"answer\": \"I donâ€™t know the answer\"}\n",
        "        return {\"answer\": result}\n",
        "\n",
        "    # Function to run the Uvicorn server on the selected port\n",
        "    def start_uvicorn():\n",
        "        uvicorn.run(app, host='0.0.0.0', port=port)\n",
        "\n",
        "    # Start the server on a separate thread\n",
        "    server_thread = threading.Thread(target=start_uvicorn, daemon=True)\n",
        "    server_thread.start()\n",
        "\n",
        "    # Set up ngrok authtoken\n",
        "    ngrok.set_auth_token(\"2g2gInvCiLEokbX8kQl0H9o2OoQ_3ixAnVMYPRC5CJ76qEmHa\")\n",
        "\n",
        "    # Establish a tunnel using HTTP protocol with the chosen port\n",
        "    public_tunnel = ngrok.connect(addr=str(port), proto=\"http\")\n",
        "    public_url = public_tunnel.public_url\n",
        "    print(f'Public URL: {public_url}')\n",
        "\n",
        "    # Example input data\n",
        "    data = {\n",
        "        \"url\": \"https://en.wikipedia.org/wiki/Generative_artificial_intelligence\",\n",
        "        \"question\": \"What are the concerns around Generative AI?\"\n",
        "    }\n",
        "\n",
        "    # Make the POST request to the FastAPI server\n",
        "    response = requests.post(f'{public_url}/question_answering', json=data)\n",
        "    print(response.json())\n"
      ],
      "metadata": {
        "id": "1yhpioACjS8x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e077b36-9f7d-413a-ac21-4f439b09e90c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "INFO:     Started server process [45587]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8002 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://aa4b-34-145-55-29.ngrok-free.app\n",
            "INFO:     34.145.55.29:0 - \"POST /question_answering HTTP/1.1\" 200 OK\n",
            "{'answer': 'Cybersecurity and Privacy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 2"
      ],
      "metadata": {
        "id": "Wf_BCNBq0Fru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn transformers torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8k4gU6Inii5",
        "outputId": "454d25da-939c-44e3-a670-b262995accdf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.111.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.37.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.11.0)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.2)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: jinja2>=2.11.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.1.3)\n",
            "Requirement already satisfied: python-multipart>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.0.9)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (5.9.0)\n",
            "Requirement already satisfied: orjson>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from fastapi) (3.10.3)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.1.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.127)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (2.6.1)\n",
            "Requirement already satisfied: idna>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from email_validator>=2.0.0->fastapi) (3.7)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.10/dist-packages (from fastapi-cli>=0.0.2->fastapi) (0.12.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.23.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.2->fastapi) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.18.2)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.6.1)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.19.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.21.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.23.0->fastapi) (1.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from transformers import pipeline\n",
        "import uvicorn\n",
        "import threading\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize the FastAPI application\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the GPT-2 text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "class ContentRequest(BaseModel):\n",
        "    topic: str\n",
        "    format: str\n",
        "\n",
        "# Function to generate text based on format\n",
        "def generate_text(topic: str, format: str) -> str:\n",
        "    # Define templates for each format with specific instructions\n",
        "    templates = {\n",
        "        'linkedin post': (\n",
        "            f\"Write an engaging LinkedIn post about {topic}. \"\n",
        "            f\"Explain why this topic is important in today's world, highlight key benefits, and encourage discussion using hashtags like #Innovation and #TechRevolution. \"\n",
        "            f\"Include emojis: ðŸš€ðŸ¤–ðŸ’¡.\"\n",
        "        ),\n",
        "        'email': (\n",
        "            f\"Write an informative email about {topic}, providing valuable insights and useful data. \"\n",
        "            f\"Include practical tips and references for further reading.\"\n",
        "        ),\n",
        "        'blog': (\n",
        "            f\"Write a blog post about {topic}, focusing on practical applications, challenges, and future prospects. \"\n",
        "            f\"Make sure to cover key aspects and add relevant examples.\"\n",
        "        )\n",
        "    }\n",
        "\n",
        "    # Check if the requested format is available\n",
        "    prompt = templates.get(format.lower())\n",
        "    if not prompt:\n",
        "        raise ValueError(\"Unsupported format. Please use 'linkedin post', 'email', or 'blog'.\")\n",
        "\n",
        "    # Generate the text using the GPT-2 pipeline\n",
        "    response = generator(\n",
        "        prompt,\n",
        "        max_length=150,  # Adjust as per your requirement\n",
        "        num_return_sequences=1,\n",
        "        temperature=0.7,       # Adjust temperature\n",
        "        top_p=0.9,             # Nucleus sampling to improve relevance\n",
        "        truncation=True,       # Ensure truncation is active\n",
        "    )\n",
        "    return response[0][\"generated_text\"].strip()\n",
        "\n",
        "# Define content generation endpoint\n",
        "@app.post('/generate_content')\n",
        "def generate_content(data: ContentRequest):\n",
        "    try:\n",
        "        result = generate_text(data.topic, data.format)\n",
        "    except ValueError as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"Error generating content: {str(e)}\")\n",
        "\n",
        "    # Return the generated content\n",
        "    return {\"text\": result}\n",
        "\n",
        "# Function to run the Uvicorn server\n",
        "def start_uvicorn():\n",
        "    uvicorn.run(app, host='0.0.0.0', port=8004)\n",
        "\n",
        "# Start the server on a separate thread\n",
        "server_thread = threading.Thread(target=start_uvicorn, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "# Set up ngrok authtoken\n",
        "ngrok.set_auth_token(\"2g2gInvCiLEokbX8kQl0H9o2OoQ_3ixAnVMYPRC5CJ76qEmHa\")\n",
        "\n",
        "# Establish a tunnel using HTTP protocol with the selected port\n",
        "public_tunnel = ngrok.connect(addr=\"8004\", proto=\"http\")\n",
        "public_url = public_tunnel.public_url\n",
        "print(f'Public URL: {public_url}')\n",
        "\n",
        "# Example input data for testing\n",
        "import requests\n",
        "\n",
        "data = {\n",
        "    \"format\": \"linkedin post\",\n",
        "    \"topic\": \"Generative AI\"\n",
        "}\n",
        "\n",
        "# Make the POST request to the FastAPI server\n",
        "response = requests.post(f'{public_url}/generate_content', json=data)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kUPsLZ6njYL",
        "outputId": "50cc8d70-d898-46d9-a59b-c05745932315"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [740]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8004): address already in use\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://9493-34-122-193-123.ngrok-free.app\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     34.122.193.123:0 - \"POST /generate_content HTTP/1.1\" 200 OK\n",
            "{'text': \"Write an engaging LinkedIn post about Generative AI. Explain why this topic is important in today's world, highlight key benefits, and encourage discussion using hashtags like #Innovation and #TechRevolution. Include emojis: ðŸš€ðŸ¤–ðŸ’¡.\\n\\nðŸ¤–ðŸ’¡. Make sure you're following our team and have your own personal Twitter account ðŸ¤–ðŸ’¡.\\n\\nWhat are the benefits of Generative AI?\\n\\nIt allows you to create the most effective workflows and execute the most effective tasks, but it also makes it easier to learn and perform. It's also a great way to learn about AI, which is why it's a great way to start learning about\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More powerful model instead of GPT-2, such as GPT-2 medium"
      ],
      "metadata": {
        "id": "_NvVk_soz5wl"
      }
    }
  ]
}